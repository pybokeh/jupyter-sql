{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a2085f7-38db-4d58-8a8d-652a8b940f02",
   "metadata": {},
   "source": [
    "# Installing \"Local\" PySpark on Windows 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050423fd-758c-462e-94cf-3c36e97e026b",
   "metadata": {},
   "source": [
    "1. Install Java 1.8 from Sun Java [site](https://www.java.com/download/ie_manual.jsp).  Include path to java.exe in your PATH environment variable.\n",
    "2. Install Python\n",
    "3. Create pyspark_dev virtual environment\n",
    "4. Activate \"pyspark_dev\" environment, then: pip install pyspark[sql] ipykernel\n",
    "5. Install kernel: python -m ipykernel install --user --name pyspark_dev --display-name \"Python (pyspark_dev)\"\n",
    "6. Set environment variables: PYSPARK_PYTHON=[path_to_python.exe] and SPARK_HOME=[path_to_site_packages/pyspark_folder]\n",
    "7. Download winutils.exe from https://github.com/cdarlint/winutils, save locally to \"hadoop/bin\" folder and then\n",
    "8. set HADOOP_HOME=[path_to_hadoop_folder]\n",
    "9. Activate python virutal environment that has jupyterlab installed and then launch: jupyter lab\n",
    "\n",
    "See this article https://phoenixnap.com/kb/install-spark-on-windows-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10937a9-14a7-475a-b6fa-829d3a37d725",
   "metadata": {},
   "source": [
    "# Installing \"Local\" PySpark on Ubuntu Linux WSL via pip in virtual environment, NOT system level installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a232f-1434-47c0-8835-cf3c8234eb45",
   "metadata": {},
   "source": [
    "1. Install Java 1.8 and set JAVA_HOME environment variable, as an example: export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64\n",
    "2. Create \"pyspark_local\" virtual environment: python3 -m venv pyspark_local\n",
    "3. pip install necessary packages: PYSPARK_HADOOP_VERSION=2 pip install pyspark pandas ipykernel\n",
    "4. Add 2 environment variables (SPARK_HOME and PYSPARK_HOME), as an example: export SPARK_HOME=/home/pybokeh/envs/pyspark_local/lib/python3.10/site-packages/pyspark and export PYSPARK_PYTHON=/home/pybokeh/envs/pyspark_local/bin/python\n",
    "5. Append SPARK_HOME to your PATH, as an example: `export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin`\n",
    "6. Activate your pyspark_local environment and then issue the \"pyspark\" command to check if everything was installed correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f0be6-2501-42a8-a91e-1bd7eee1fb0d",
   "metadata": {},
   "source": [
    "Official installation [instructions](https://spark.apache.org/docs/latest/api/python/getting_started/install.html) from spark documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "664504f6-bbac-466e-982d-42c62fb82a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 13:26:23 WARN Utils: Your hostname, pybokeh-Lemur resolves to a loopback address: 127.0.1.1; using 192.168.1.105 instead (on interface wlp2s0)\n",
      "23/03/05 13:26:23 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 13:26:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"local_pyspark\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c991e9ac-bf9e-42e2-89ae-bdad142eaf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/cars.csv', header=True, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bca9f97-40b0-4d66-9f6a-235b7125e70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|                 Car| MPG|Cylinders|Displacement|Horsepower|Weight|Acceleration|Model|Origin|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "|Chevrolet Chevell...|18.0|        8|       307.0|     130.0| 3504.|        12.0|   70|    US|\n",
      "|   Buick Skylark 320|15.0|        8|       350.0|     165.0| 3693.|        11.5|   70|    US|\n",
      "|  Plymouth Satellite|18.0|        8|       318.0|     150.0| 3436.|        11.0|   70|    US|\n",
      "|       AMC Rebel SST|16.0|        8|       304.0|     150.0| 3433.|        12.0|   70|    US|\n",
      "|         Ford Torino|17.0|        8|       302.0|     140.0| 3449.|        10.5|   70|    US|\n",
      "+--------------------+----+---------+------------+----------+------+------------+-----+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.10 (PySpark 3.3.2)",
   "language": "python",
   "name": "pyspark_local"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
